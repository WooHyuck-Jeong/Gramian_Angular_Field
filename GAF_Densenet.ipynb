{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\JWH\\AppData\\Local\\Temp/ipykernel_8508/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15068201377496061323\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18266593192872506552\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/3.10/GAF/GAF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 200, 200, 2)\n",
      "(1440,)\n",
      "(360, 200, 200, 2)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 200, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 206, 206, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 100, 100, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 100, 100, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 100, 100, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 102, 102, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 50, 50, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 50, 50, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 50, 50, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 50, 50, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 50, 50, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 50, 50, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 50, 50, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 50, 50, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 50, 50, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 50, 50, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 50, 50, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 50, 50, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 50, 50, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 50, 50, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 50, 50, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 50, 50, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 50, 50, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 50, 50, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 50, 50, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 50, 50, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 50, 50, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 50, 50, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 50, 50, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 50, 50, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 50, 50, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 50, 50, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 50, 50, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 50, 50, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 50, 50, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 25, 25, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 25, 25, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 25, 25, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 25, 25, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 25, 25, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 25, 25, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 25, 25, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 25, 25, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 25, 25, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 25, 25, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 25, 25, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 25, 25, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 25, 25, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 25, 25, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 25, 25, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 25, 25, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 25, 25, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 25, 25, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 25, 25, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 25, 25, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 25, 25, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 25, 25, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 25, 25, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 25, 25, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 25, 25, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 25, 25, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 25, 25, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 25, 25, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 25, 25, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 25, 25, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 25, 25, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 25, 25, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 25, 25, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 25, 25, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 25, 25, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 25, 25, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 25, 25, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 25, 25, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 25, 25, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 25, 25, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 25, 25, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 25, 25, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 25, 25, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 25, 25, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 25, 25, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 25, 25, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 25, 25, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 25, 25, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 25, 25, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 25, 25, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 25, 25, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 25, 25, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 25, 25, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 25, 25, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 25, 25, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 25, 25, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 25, 25, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 25, 25, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 25, 25, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 25, 25, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 25, 25, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 25, 25, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 25, 25, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 12, 12, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 12, 12, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 12, 12, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 12, 12, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 12, 12, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 12, 12, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 12, 12, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 12, 12, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 12, 12, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 12, 12, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 12, 12, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 12, 12, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 12, 12, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 12, 12, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 12, 12, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 12, 12, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 12, 12, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 12, 12, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 12, 12, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 12, 12, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 12, 12, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 12, 12, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 12, 12, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 12, 12, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 12, 12, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 12, 12, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 12, 12, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 12, 12, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 12, 12, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 12, 12, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 12, 12, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 12, 12, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 12, 12, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 12, 12, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 12, 12, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 12, 12, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 12, 12, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 12, 12, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 12, 12, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 12, 12, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 12, 12, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 12, 12, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 12, 12, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 12, 12, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 12, 12, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 12, 12, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 12, 12, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 12, 12, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 12, 12, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 12, 12, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 12, 12, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 12, 12, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 12, 12, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 12, 12, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 12, 12, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 12, 12, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 12, 12, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 12, 12, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 12, 12, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 12, 12, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 12, 12, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 12, 12, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 12, 12, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 12, 12, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 12, 12, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 12, 12, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 12, 12, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 12, 12, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 12, 12, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 12, 12, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 12, 12, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 12, 12, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 12, 12, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 12, 12, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 12, 12, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 12, 12, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 12, 12, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 12, 12, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 12, 12, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 12, 12, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 12, 12, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 12, 12, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 12, 12, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 12, 12, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 12, 12, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 12, 12, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 12, 12, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 12, 12, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 12, 12, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 12, 12, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 12, 12, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 12, 12, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 12, 12, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 12, 12, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 12, 12, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 12, 12, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 12, 12, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 12, 12, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 12, 12, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 12, 12, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 12, 12, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 12, 12, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 12, 12, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 12, 12, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 12, 12, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 12, 12, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 6, 6, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 6, 6, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 6, 6, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 6, 6, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 6, 6, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 6, 6, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 6, 6, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 6, 6, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 6, 6, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 6, 6, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 6, 6, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 6, 6, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 6, 6, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 6, 6, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 6, 6, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 6, 6, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 6, 6, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 6, 6, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 6, 6, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 6, 6, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 6, 6, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 6, 6, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 6, 6, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 6, 6, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 6, 6, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 6, 6, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 6, 6, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 6, 6, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 6, 6, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 6, 6, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 6, 6, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 6, 6, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 6, 6, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 6, 6, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 6, 6, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 6, 6, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 6, 6, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 6, 6, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 6, 6, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 6, 6, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 6, 6, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 6, 6, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 6, 6, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 6, 6, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 6, 6, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 6, 6, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 6, 6, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 6, 6, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 6, 6, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 6, 6, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 6, 6, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 6, 6, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 6, 6, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 6, 6, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 6, 6, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 6, 6, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 6, 6, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 6, 6, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 6, 6, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 6, 6, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 6, 6, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 6, 6, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 6, 6, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 6, 6, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 6, 6, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 6, 6, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 6, 6, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 6, 6, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 6, 6, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 6, 6, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 6, 6, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 6, 6, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 6, 6, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(200, 200, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n",
      "180\n",
      "1620\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1620]\n",
    "train = np.reshape(train, 1620)\n",
    "test = test_[0:180]\n",
    "test = np.reshape(test, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4   37   58   71   95  101  108  111  116  127  140  141  146  172\n",
      "  179  185  204  206  229  237  241  281  284  287  299  324  344  351\n",
      "  359  367  372  386  389  396  410  412  415  416  431  449  452  454\n",
      "  500  514  517  527  540  546  553  556  572  595  602  620  623  635\n",
      "  637  648  649  658  666  671  674  677  680  681  683  687  698  700\n",
      "  701  704  720  736  739  754  756  757  762  797  802  816  820  833\n",
      "  848  870  879  882  883  915  945  946  954  970  972  975  978  995\n",
      " 1002 1025 1027 1028 1046 1058 1072 1088 1093 1094 1104 1114 1124 1138\n",
      " 1144 1151 1152 1156 1157 1167 1168 1195 1215 1257 1261 1267 1281 1303\n",
      " 1305 1320 1340 1343 1348 1375 1377 1399 1411 1418 1425 1427 1433 1442\n",
      " 1449 1466 1475 1497 1506 1517 1524 1543 1552 1560 1567 1570 1573 1578\n",
      " 1589 1602 1611 1616 1618 1625 1634 1637 1645 1648 1664 1668 1669 1673\n",
      " 1702 1717 1718 1721 1735 1740 1741 1749 1758 1760 1771 1779]\n",
      "[   0    1    2 ... 1797 1798 1799]\n",
      "['H_401' 'H_179' 'H_239' 'N_25' 'N_232' 'P_340' 'N_70' 'P_593' 'P_232'\n",
      " 'P_533' 'N_391' 'P_564' 'N_60' 'H_502' 'P_396' 'N_249' 'H_8' 'N_214'\n",
      " 'N_141' 'P_552' 'H_425' 'H_298' 'N_12' 'N_248' 'N_596' 'P_501' 'P_346'\n",
      " 'P_370' 'H_543' 'H_397' 'P_583' 'N_263' 'H_287' 'P_161' 'P_536' 'P_27'\n",
      " 'P_241' 'N_335' 'H_173' 'H_585' 'P_419' 'H_251' 'H_121' 'N_319' 'N_448'\n",
      " 'H_269' 'P_251' 'N_163' 'N_390' 'P_508' 'P_486' 'P_432' 'H_254' 'H_54'\n",
      " 'P_512' 'N_325' 'N_185' 'H_80' 'N_133' 'N_349' 'N_393' 'P_12' 'N_93'\n",
      " 'H_194' 'P_153' 'P_443' 'N_549' 'N_592' 'P_9' 'N_88' 'H_22' 'H_591'\n",
      " 'H_43' 'H_136' 'P_490' 'P_144' 'N_376' 'N_472' 'N_38' 'P_320' 'N_567'\n",
      " 'H_579' 'P_560' 'N_411' 'H_84' 'H_214' 'H_419' 'H_508' 'N_39' 'H_426'\n",
      " 'P_311' 'N_187' 'N_154' 'H_546' 'N_54' 'N_429' 'P_175' 'P_333' 'H_143'\n",
      " 'P_353' 'H_503' 'H_49' 'H_117' 'P_545' 'N_341' 'P_158' 'P_312' 'H_333'\n",
      " 'N_301' 'H_17' 'H_382' 'P_414' 'P_376' 'N_508' 'N_124' 'H_93' 'P_502'\n",
      " 'N_165' 'H_362' 'N_414' 'P_494' 'N_558' 'P_115' 'H_203' 'P_128' 'P_313'\n",
      " 'P_199' 'H_342' 'N_452' 'P_592' 'P_173' 'H_495' 'P_140' 'P_328' 'N_17'\n",
      " 'P_529' 'H_295' 'N_575' 'H_550' 'H_83' 'H_438' 'H_575' 'N_481' 'N_329'\n",
      " 'P_168' 'N_58' 'H_119' 'P_246' 'H_467' 'H_498' 'H_52' 'P_74' 'H_242'\n",
      " 'N_216' 'N_346' 'N_243' 'P_248' 'P_71' 'P_318' 'P_223' 'P_263' 'P_191'\n",
      " 'P_234' 'P_7' 'N_532' 'P_438' 'P_457' 'N_23' 'H_398' 'H_360' 'N_16'\n",
      " 'N_155' 'H_236' 'H_114' 'H_480' 'N_100' 'H_405' 'P_418' 'H_458' 'H_413']\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('E:/3.10/GAF' + '/' + 'train')\n",
    "os.mkdir('E:/3.10/GAF' + '/' + 'test')\n",
    "os.mkdir('E:/3.10/GAF' + '/' + 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296,) (144,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JWH\\anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 32s 115ms/step - loss: 0.3633 - accuracy: 0.7454 - val_loss: 1.5856 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 18s 112ms/step - loss: 0.2870 - accuracy: 0.8179 - val_loss: 3.0798 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 19s 118ms/step - loss: 0.2278 - accuracy: 0.8503 - val_loss: 0.7032 - val_accuracy: 0.6181\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 21s 132ms/step - loss: 0.2033 - accuracy: 0.8765 - val_loss: 0.4030 - val_accuracy: 0.8056\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 19s 115ms/step - loss: 0.1782 - accuracy: 0.8904 - val_loss: 0.4468 - val_accuracy: 0.6736\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 18s 113ms/step - loss: 0.1399 - accuracy: 0.9167 - val_loss: 0.4474 - val_accuracy: 0.7639\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 19s 117ms/step - loss: 0.1211 - accuracy: 0.9336 - val_loss: 0.3688 - val_accuracy: 0.8264\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 18s 109ms/step - loss: 0.1050 - accuracy: 0.9360 - val_loss: 0.2504 - val_accuracy: 0.8542\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0923 - accuracy: 0.9529 - val_loss: 0.3249 - val_accuracy: 0.8403\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0610 - accuracy: 0.9753 - val_loss: 0.4715 - val_accuracy: 0.8264\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0668 - accuracy: 0.9684 - val_loss: 0.4843 - val_accuracy: 0.7847\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0722 - accuracy: 0.9630 - val_loss: 0.7691 - val_accuracy: 0.7083\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0447 - accuracy: 0.9815 - val_loss: 0.3378 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0437 - accuracy: 0.9761 - val_loss: 0.2917 - val_accuracy: 0.8681\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 16s 100ms/step - loss: 0.0393 - accuracy: 0.9823 - val_loss: 0.2859 - val_accuracy: 0.8611\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 16s 101ms/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.2584 - val_accuracy: 0.8819\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 16s 101ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.3182 - val_accuracy: 0.8750\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 16s 100ms/step - loss: 0.0292 - accuracy: 0.9877 - val_loss: 0.3571 - val_accuracy: 0.8472\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0351 - accuracy: 0.9861 - val_loss: 0.4551 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.4215 - val_accuracy: 0.8333\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0287 - accuracy: 0.9907 - val_loss: 0.3141 - val_accuracy: 0.8403\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.3105 - val_accuracy: 0.8472\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0146 - accuracy: 0.9961 - val_loss: 0.2982 - val_accuracy: 0.8542\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0124 - accuracy: 0.9992 - val_loss: 0.2824 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0177 - accuracy: 0.9985 - val_loss: 0.3009 - val_accuracy: 0.8542\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 18s 109ms/step - loss: 0.0243 - accuracy: 0.9907 - val_loss: 0.3138 - val_accuracy: 0.8611\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.2990 - val_accuracy: 0.8542\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0152 - accuracy: 0.9961 - val_loss: 0.2876 - val_accuracy: 0.8611\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0186 - accuracy: 0.9969 - val_loss: 0.3003 - val_accuracy: 0.8472\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.2883 - val_accuracy: 0.8611\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0144 - accuracy: 0.9985 - val_loss: 0.2898 - val_accuracy: 0.8611\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.2974 - val_accuracy: 0.8611\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3107 - val_accuracy: 0.8542\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0361 - accuracy: 0.9869 - val_loss: 0.3003 - val_accuracy: 0.8542\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.2916 - val_accuracy: 0.8611\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.2909 - val_accuracy: 0.8611\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.2886 - val_accuracy: 0.8750\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0247 - accuracy: 0.9938 - val_loss: 0.2954 - val_accuracy: 0.8611\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0152 - accuracy: 0.9961 - val_loss: 0.2937 - val_accuracy: 0.8819\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.2946 - val_accuracy: 0.8611\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.3000 - val_accuracy: 0.8611\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.8611\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.2982 - val_accuracy: 0.8611\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0147 - accuracy: 0.9977 - val_loss: 0.2969 - val_accuracy: 0.8611\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.2962 - val_accuracy: 0.8611\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0102 - accuracy: 0.9992 - val_loss: 0.3061 - val_accuracy: 0.8611\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0237 - accuracy: 0.9931 - val_loss: 0.2992 - val_accuracy: 0.8611\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.2997 - val_accuracy: 0.8611\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3003 - val_accuracy: 0.8611\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0102 - accuracy: 0.9992 - val_loss: 0.2955 - val_accuracy: 0.8611\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.2952 - val_accuracy: 0.8681\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0151 - accuracy: 0.9938 - val_loss: 0.2981 - val_accuracy: 0.8611\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0110 - accuracy: 0.9992 - val_loss: 0.2985 - val_accuracy: 0.8611\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0129 - accuracy: 0.9946 - val_loss: 0.2944 - val_accuracy: 0.8681\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.2999 - val_accuracy: 0.8611\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.2957 - val_accuracy: 0.8681\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.2950 - val_accuracy: 0.8611\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0088 - accuracy: 0.9992 - val_loss: 0.2971 - val_accuracy: 0.8611\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 0.3078 - val_accuracy: 0.8681\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.3060 - val_accuracy: 0.8611\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0094 - accuracy: 0.9992 - val_loss: 0.3013 - val_accuracy: 0.8611\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0255 - accuracy: 0.9961 - val_loss: 0.2975 - val_accuracy: 0.8611\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0147 - accuracy: 0.9992 - val_loss: 0.2949 - val_accuracy: 0.8681\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.3017 - val_accuracy: 0.8611\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.2968 - val_accuracy: 0.8611\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.3028 - val_accuracy: 0.8611\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0087 - accuracy: 0.9992 - val_loss: 0.3016 - val_accuracy: 0.8611\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0099 - accuracy: 0.9992 - val_loss: 0.3038 - val_accuracy: 0.8611\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0133 - accuracy: 0.9946 - val_loss: 0.2962 - val_accuracy: 0.8681\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.3066 - val_accuracy: 0.8681\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.2997 - val_accuracy: 0.8611\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.3043 - val_accuracy: 0.8611\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.2955 - val_accuracy: 0.8611\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.3076 - val_accuracy: 0.8681\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.3017 - val_accuracy: 0.8611\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0253 - accuracy: 0.9946 - val_loss: 0.3009 - val_accuracy: 0.8611\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0207 - accuracy: 0.9946 - val_loss: 0.3027 - val_accuracy: 0.8611\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 18s 109ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.3010 - val_accuracy: 0.8611\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 18s 108ms/step - loss: 0.0074 - accuracy: 0.9992 - val_loss: 0.3037 - val_accuracy: 0.8611\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9977 - val_loss: 0.2954 - val_accuracy: 0.8681\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.2940 - val_accuracy: 0.8681\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0184 - accuracy: 0.9938 - val_loss: 0.2977 - val_accuracy: 0.8611\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.8611\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.2909 - val_accuracy: 0.8681\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.2961 - val_accuracy: 0.8681\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.2953 - val_accuracy: 0.8681\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.2958 - val_accuracy: 0.8681\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.3017 - val_accuracy: 0.8611\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0104 - accuracy: 0.9961 - val_loss: 0.2957 - val_accuracy: 0.8681\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.2966 - val_accuracy: 0.8611\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.2959 - val_accuracy: 0.8681\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.2984 - val_accuracy: 0.8611\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.2965 - val_accuracy: 0.8611\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.8681\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0097 - accuracy: 0.9992 - val_loss: 0.3048 - val_accuracy: 0.8611\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0106 - accuracy: 0.9992 - val_loss: 0.3052 - val_accuracy: 0.8611\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.3022 - val_accuracy: 0.8611\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2989 - val_accuracy: 0.8611\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0162 - accuracy: 0.9946 - val_loss: 0.2992 - val_accuracy: 0.8611\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.2992 - val_accuracy: 0.8611\n",
      "Score for fold 1: loss of 0.2991652190685272; accuracy of 86.11111044883728%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 116ms/step - loss: 0.3704 - accuracy: 0.7469 - val_loss: 3.0113 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.3133 - accuracy: 0.7870 - val_loss: 1.6871 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.2503 - accuracy: 0.8434 - val_loss: 0.3491 - val_accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1981 - accuracy: 0.8850 - val_loss: 0.2826 - val_accuracy: 0.8264\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1843 - accuracy: 0.8897 - val_loss: 0.6068 - val_accuracy: 0.7292\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1433 - accuracy: 0.9182 - val_loss: 0.4809 - val_accuracy: 0.6458\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1153 - accuracy: 0.9375 - val_loss: 0.6275 - val_accuracy: 0.7847\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1038 - accuracy: 0.9421 - val_loss: 0.6890 - val_accuracy: 0.7639\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0884 - accuracy: 0.9537 - val_loss: 0.8223 - val_accuracy: 0.7014\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0724 - accuracy: 0.9660 - val_loss: 0.5065 - val_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0946 - accuracy: 0.9537 - val_loss: 0.3701 - val_accuracy: 0.8333\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0674 - accuracy: 0.9614 - val_loss: 0.2654 - val_accuracy: 0.8750\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0471 - accuracy: 0.9784 - val_loss: 0.6300 - val_accuracy: 0.7639\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0464 - accuracy: 0.9838 - val_loss: 0.3729 - val_accuracy: 0.8333\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0592 - accuracy: 0.9715 - val_loss: 0.2221 - val_accuracy: 0.8681\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 19s 117ms/step - loss: 0.0432 - accuracy: 0.9853 - val_loss: 0.2113 - val_accuracy: 0.8819\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.2308 - val_accuracy: 0.8889\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0267 - accuracy: 0.9884 - val_loss: 0.2591 - val_accuracy: 0.8472\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0222 - accuracy: 0.9907 - val_loss: 0.2269 - val_accuracy: 0.8611\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0319 - accuracy: 0.9853 - val_loss: 0.2262 - val_accuracy: 0.8542\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0284 - accuracy: 0.9877 - val_loss: 0.1977 - val_accuracy: 0.8750\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.2115 - val_accuracy: 0.8542\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0245 - accuracy: 0.9892 - val_loss: 0.2182 - val_accuracy: 0.8542\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0199 - accuracy: 0.9977 - val_loss: 0.2089 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.2194 - val_accuracy: 0.8611\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.2158 - val_accuracy: 0.8542\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2129 - val_accuracy: 0.8681\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.2175 - val_accuracy: 0.8472\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.1999 - val_accuracy: 0.8750\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.2043 - val_accuracy: 0.8750\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.2136 - val_accuracy: 0.8611\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0126 - accuracy: 0.9977 - val_loss: 0.2095 - val_accuracy: 0.8819\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.2129 - val_accuracy: 0.8750\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.2076 - val_accuracy: 0.8819\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.2174 - val_accuracy: 0.8611\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0119 - accuracy: 0.9992 - val_loss: 0.2142 - val_accuracy: 0.8750\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0134 - accuracy: 0.9985 - val_loss: 0.2158 - val_accuracy: 0.8750\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.2252 - val_accuracy: 0.8542\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.2374 - val_accuracy: 0.8542\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0148 - accuracy: 0.9977 - val_loss: 0.2187 - val_accuracy: 0.8681\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.2201 - val_accuracy: 0.8681\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.2208 - val_accuracy: 0.8681\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.2219 - val_accuracy: 0.8681\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0170 - accuracy: 0.9938 - val_loss: 0.2192 - val_accuracy: 0.8681\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.2204 - val_accuracy: 0.8681\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.2189 - val_accuracy: 0.8681\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.2226 - val_accuracy: 0.8611\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.2198 - val_accuracy: 0.8681\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0136 - accuracy: 0.9977 - val_loss: 0.2191 - val_accuracy: 0.8681\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0102 - accuracy: 0.9992 - val_loss: 0.2188 - val_accuracy: 0.8681\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.2176 - val_accuracy: 0.8750\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0147 - accuracy: 0.9992 - val_loss: 0.2200 - val_accuracy: 0.8681\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.2184 - val_accuracy: 0.8750\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2195 - val_accuracy: 0.8681\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0210 - accuracy: 0.9961 - val_loss: 0.2211 - val_accuracy: 0.8681\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.8681\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.2190 - val_accuracy: 0.8681\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.2195 - val_accuracy: 0.8681\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.2199 - val_accuracy: 0.8681\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2175 - val_accuracy: 0.8750\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2185 - val_accuracy: 0.8681\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.2181 - val_accuracy: 0.8681\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.2183 - val_accuracy: 0.8681\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 0.8750\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.2176 - val_accuracy: 0.8750\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0115 - accuracy: 0.9992 - val_loss: 0.2205 - val_accuracy: 0.8681\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.2170 - val_accuracy: 0.8750\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0163 - accuracy: 0.9961 - val_loss: 0.2182 - val_accuracy: 0.8750\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.8681\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.2192 - val_accuracy: 0.8681\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0113 - accuracy: 0.9977 - val_loss: 0.2152 - val_accuracy: 0.8750\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0085 - accuracy: 0.9992 - val_loss: 0.2157 - val_accuracy: 0.8819\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.2170 - val_accuracy: 0.8750\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0221 - accuracy: 0.9907 - val_loss: 0.2194 - val_accuracy: 0.8681\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0097 - accuracy: 0.9992 - val_loss: 0.2160 - val_accuracy: 0.8750\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.8681\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.2175 - val_accuracy: 0.8750\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0139 - accuracy: 0.9977 - val_loss: 0.2186 - val_accuracy: 0.8750\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.8750\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.2171 - val_accuracy: 0.8750\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0225 - accuracy: 0.9946 - val_loss: 0.2168 - val_accuracy: 0.8750\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.2178 - val_accuracy: 0.8750\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0138 - accuracy: 0.9977 - val_loss: 0.2171 - val_accuracy: 0.8750\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.8750\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.2174 - val_accuracy: 0.8750\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.2188 - val_accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.2199 - val_accuracy: 0.8681\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2169 - val_accuracy: 0.8750\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.2161 - val_accuracy: 0.8819\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.2184 - val_accuracy: 0.8681\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.2189 - val_accuracy: 0.8681\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 0.2189 - val_accuracy: 0.8681\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 0.8750\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.2162 - val_accuracy: 0.8750\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.2172 - val_accuracy: 0.8681\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0161 - accuracy: 0.9923 - val_loss: 0.2193 - val_accuracy: 0.8681\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0104 - accuracy: 0.9992 - val_loss: 0.2190 - val_accuracy: 0.8681\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0134 - accuracy: 0.9985 - val_loss: 0.2177 - val_accuracy: 0.8681\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0111 - accuracy: 0.9992 - val_loss: 0.2188 - val_accuracy: 0.8750\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.2184 - val_accuracy: 0.8750\n",
      "Score for fold 2: loss of 0.21841709315776825; accuracy of 87.5%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 117ms/step - loss: 0.3876 - accuracy: 0.7122 - val_loss: 3.2052 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2806 - accuracy: 0.8248 - val_loss: 1.2587 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.2547 - accuracy: 0.8441 - val_loss: 0.4967 - val_accuracy: 0.6181\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.2078 - accuracy: 0.8634 - val_loss: 0.4727 - val_accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.2050 - accuracy: 0.8665 - val_loss: 0.6659 - val_accuracy: 0.6736\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.1582 - accuracy: 0.9151 - val_loss: 0.5366 - val_accuracy: 0.7569\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1419 - accuracy: 0.9182 - val_loss: 0.2013 - val_accuracy: 0.8750\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.1227 - accuracy: 0.9383 - val_loss: 1.1435 - val_accuracy: 0.6181\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.1004 - accuracy: 0.9452 - val_loss: 0.4920 - val_accuracy: 0.8194\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.1025 - accuracy: 0.9475 - val_loss: 0.2719 - val_accuracy: 0.8542\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0704 - accuracy: 0.9645 - val_loss: 0.3601 - val_accuracy: 0.8542\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0739 - accuracy: 0.9599 - val_loss: 0.4409 - val_accuracy: 0.8333\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0494 - accuracy: 0.9815 - val_loss: 0.4180 - val_accuracy: 0.8403\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0502 - accuracy: 0.9776 - val_loss: 0.3759 - val_accuracy: 0.8333\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0555 - accuracy: 0.9707 - val_loss: 0.4044 - val_accuracy: 0.8542\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0468 - accuracy: 0.9807 - val_loss: 0.3376 - val_accuracy: 0.8333\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0448 - accuracy: 0.9792 - val_loss: 0.2703 - val_accuracy: 0.8681\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0496 - accuracy: 0.9815 - val_loss: 0.4587 - val_accuracy: 0.8403\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0352 - accuracy: 0.9869 - val_loss: 0.2666 - val_accuracy: 0.8681\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0428 - accuracy: 0.9838 - val_loss: 0.4227 - val_accuracy: 0.7778\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 0.2447 - val_accuracy: 0.8681\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 18s 111ms/step - loss: 0.0268 - accuracy: 0.9884 - val_loss: 0.2458 - val_accuracy: 0.8819\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 18s 109ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.2542 - val_accuracy: 0.8681\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0156 - accuracy: 0.9977 - val_loss: 0.2621 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0290 - accuracy: 0.9900 - val_loss: 0.2573 - val_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.2573 - val_accuracy: 0.8819\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.2673 - val_accuracy: 0.8681\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 18s 110ms/step - loss: 0.0226 - accuracy: 0.9961 - val_loss: 0.2580 - val_accuracy: 0.8819\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0146 - accuracy: 0.9961 - val_loss: 0.2643 - val_accuracy: 0.8819\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.2500 - val_accuracy: 0.8819\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0219 - accuracy: 0.9946 - val_loss: 0.2571 - val_accuracy: 0.8819\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.2513 - val_accuracy: 0.8819\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0153 - accuracy: 0.9992 - val_loss: 0.2589 - val_accuracy: 0.8889\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0266 - accuracy: 0.9946 - val_loss: 0.2529 - val_accuracy: 0.8819\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.2457 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 18s 109ms/step - loss: 0.0133 - accuracy: 0.9977 - val_loss: 0.2482 - val_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0147 - accuracy: 0.9977 - val_loss: 0.2413 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.2437 - val_accuracy: 0.8819\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.2473 - val_accuracy: 0.8889\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0137 - accuracy: 0.9977 - val_loss: 0.2462 - val_accuracy: 0.8958\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0160 - accuracy: 0.9961 - val_loss: 0.2463 - val_accuracy: 0.8819\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 0.2460 - val_accuracy: 0.8819\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.2480 - val_accuracy: 0.8819\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0115 - accuracy: 0.9992 - val_loss: 0.2478 - val_accuracy: 0.8819\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0182 - accuracy: 0.9961 - val_loss: 0.2472 - val_accuracy: 0.8819\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.2479 - val_accuracy: 0.8819\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9992 - val_loss: 0.2498 - val_accuracy: 0.8819\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0161 - accuracy: 0.9977 - val_loss: 0.2513 - val_accuracy: 0.8819\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0188 - accuracy: 0.9961 - val_loss: 0.2491 - val_accuracy: 0.8819\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.2496 - val_accuracy: 0.8819\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.2500 - val_accuracy: 0.8819\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.2505 - val_accuracy: 0.8819\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0148 - accuracy: 0.9985 - val_loss: 0.2504 - val_accuracy: 0.8819\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.2524 - val_accuracy: 0.8819\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9992 - val_loss: 0.2481 - val_accuracy: 0.8819\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.2495 - val_accuracy: 0.8819\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.8819\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.2499 - val_accuracy: 0.8819\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.2507 - val_accuracy: 0.8819\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.2527 - val_accuracy: 0.8819\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.2506 - val_accuracy: 0.8819\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0119 - accuracy: 0.9992 - val_loss: 0.2519 - val_accuracy: 0.8819\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.8819\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.2506 - val_accuracy: 0.8819\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 0.2498 - val_accuracy: 0.8819\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.2496 - val_accuracy: 0.8819\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 0.2524 - val_accuracy: 0.8819\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.2531 - val_accuracy: 0.8819\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2536 - val_accuracy: 0.8819\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0135 - accuracy: 0.9977 - val_loss: 0.2498 - val_accuracy: 0.8889\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.2526 - val_accuracy: 0.8819\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0155 - accuracy: 0.9977 - val_loss: 0.2496 - val_accuracy: 0.8819\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0109 - accuracy: 0.9992 - val_loss: 0.2515 - val_accuracy: 0.8819\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0125 - accuracy: 0.9992 - val_loss: 0.2522 - val_accuracy: 0.8819\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9992 - val_loss: 0.2513 - val_accuracy: 0.8819\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0203 - accuracy: 0.9969 - val_loss: 0.2509 - val_accuracy: 0.8819\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.2520 - val_accuracy: 0.8819\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0198 - accuracy: 0.9961 - val_loss: 0.2530 - val_accuracy: 0.8819\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0159 - accuracy: 0.9931 - val_loss: 0.2517 - val_accuracy: 0.8819\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.2520 - val_accuracy: 0.8819\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9992 - val_loss: 0.2527 - val_accuracy: 0.8819\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.2537 - val_accuracy: 0.8819\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.2519 - val_accuracy: 0.8819\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9985 - val_loss: 0.2540 - val_accuracy: 0.8819\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0158 - accuracy: 0.9985 - val_loss: 0.2524 - val_accuracy: 0.8819\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.8819\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.2521 - val_accuracy: 0.8819\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.2535 - val_accuracy: 0.8819\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.2527 - val_accuracy: 0.8819\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.2519 - val_accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.2526 - val_accuracy: 0.8819\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9992 - val_loss: 0.2519 - val_accuracy: 0.8889\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0182 - accuracy: 0.9961 - val_loss: 0.2529 - val_accuracy: 0.8819\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.2548 - val_accuracy: 0.8819\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.2535 - val_accuracy: 0.8819\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0118 - accuracy: 0.9977 - val_loss: 0.2536 - val_accuracy: 0.8819\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.2515 - val_accuracy: 0.8819\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0160 - accuracy: 0.9961 - val_loss: 0.2495 - val_accuracy: 0.8819\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.2518 - val_accuracy: 0.8819\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.2521 - val_accuracy: 0.8889\n",
      "Score for fold 3: loss of 0.2520538568496704; accuracy of 88.88888955116272%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 115ms/step - loss: 0.3538 - accuracy: 0.7546 - val_loss: 2.9090 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.3150 - accuracy: 0.8002 - val_loss: 4.2278 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2519 - accuracy: 0.8380 - val_loss: 0.5344 - val_accuracy: 0.7083\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.2197 - accuracy: 0.8634 - val_loss: 0.4404 - val_accuracy: 0.7361\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1908 - accuracy: 0.8835 - val_loss: 0.5545 - val_accuracy: 0.7569\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1477 - accuracy: 0.9174 - val_loss: 0.2397 - val_accuracy: 0.8750\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1352 - accuracy: 0.9259 - val_loss: 0.1876 - val_accuracy: 0.8889\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1116 - accuracy: 0.9429 - val_loss: 0.3219 - val_accuracy: 0.8125\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0909 - accuracy: 0.9514 - val_loss: 0.2504 - val_accuracy: 0.8542\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0829 - accuracy: 0.9599 - val_loss: 0.3199 - val_accuracy: 0.8125\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1064 - accuracy: 0.9475 - val_loss: 0.3202 - val_accuracy: 0.8333\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0681 - accuracy: 0.9606 - val_loss: 0.2752 - val_accuracy: 0.8611\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0720 - accuracy: 0.9676 - val_loss: 0.3037 - val_accuracy: 0.8472\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0432 - accuracy: 0.9830 - val_loss: 0.2741 - val_accuracy: 0.8889\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0426 - accuracy: 0.9830 - val_loss: 0.3009 - val_accuracy: 0.8264\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0495 - accuracy: 0.9792 - val_loss: 0.2288 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0445 - accuracy: 0.9838 - val_loss: 0.4273 - val_accuracy: 0.7778\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0260 - accuracy: 0.9923 - val_loss: 0.2457 - val_accuracy: 0.9097\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0365 - accuracy: 0.9877 - val_loss: 0.3472 - val_accuracy: 0.8611\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0280 - accuracy: 0.9892 - val_loss: 0.2170 - val_accuracy: 0.9028\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0282 - accuracy: 0.9861 - val_loss: 0.2586 - val_accuracy: 0.8958\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.2454 - val_accuracy: 0.9028\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0165 - accuracy: 0.9977 - val_loss: 0.2442 - val_accuracy: 0.9028\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0198 - accuracy: 0.9946 - val_loss: 0.2418 - val_accuracy: 0.8889\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0231 - accuracy: 0.9900 - val_loss: 0.2375 - val_accuracy: 0.8889\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0159 - accuracy: 0.9977 - val_loss: 0.2331 - val_accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0143 - accuracy: 0.9977 - val_loss: 0.2368 - val_accuracy: 0.8958\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.2443 - val_accuracy: 0.8958\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0211 - accuracy: 0.9946 - val_loss: 0.2361 - val_accuracy: 0.8958\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.2321 - val_accuracy: 0.8819\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0283 - accuracy: 0.9884 - val_loss: 0.2446 - val_accuracy: 0.8958\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.2350 - val_accuracy: 0.8819\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0154 - accuracy: 0.9992 - val_loss: 0.2489 - val_accuracy: 0.8958\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.2413 - val_accuracy: 0.8958\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.2420 - val_accuracy: 0.8819\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 0.2375 - val_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0181 - accuracy: 0.9946 - val_loss: 0.2451 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.2592 - val_accuracy: 0.8958\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.2431 - val_accuracy: 0.8958\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.8889\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.2464 - val_accuracy: 0.8889\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.2446 - val_accuracy: 0.8889\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.2449 - val_accuracy: 0.8889\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.2443 - val_accuracy: 0.8889\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.2476 - val_accuracy: 0.8958\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0140 - accuracy: 0.9985 - val_loss: 0.2457 - val_accuracy: 0.8889\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.2454 - val_accuracy: 0.8889\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0104 - accuracy: 0.9992 - val_loss: 0.2467 - val_accuracy: 0.8889\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.2448 - val_accuracy: 0.8889\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.2439 - val_accuracy: 0.8819\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2446 - val_accuracy: 0.8889\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.2447 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.2484 - val_accuracy: 0.8958\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0140 - accuracy: 0.9969 - val_loss: 0.2461 - val_accuracy: 0.8958\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.2434 - val_accuracy: 0.8889\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.2446 - val_accuracy: 0.8889\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.2462 - val_accuracy: 0.8889\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.2438 - val_accuracy: 0.8750\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.2433 - val_accuracy: 0.8889\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.2440 - val_accuracy: 0.8889\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0120 - accuracy: 0.9992 - val_loss: 0.2448 - val_accuracy: 0.8750\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0140 - accuracy: 0.9977 - val_loss: 0.2455 - val_accuracy: 0.8889\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 0.2456 - val_accuracy: 0.8889\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.2435 - val_accuracy: 0.8819\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0135 - accuracy: 0.9969 - val_loss: 0.2415 - val_accuracy: 0.8819\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.8889\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.2463 - val_accuracy: 0.8958\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0228 - accuracy: 0.9938 - val_loss: 0.2437 - val_accuracy: 0.8819\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.2414 - val_accuracy: 0.8819\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.2449 - val_accuracy: 0.8889\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.2451 - val_accuracy: 0.8889\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.2452 - val_accuracy: 0.8889\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0158 - accuracy: 0.9985 - val_loss: 0.2447 - val_accuracy: 0.8819\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0085 - accuracy: 0.9992 - val_loss: 0.2472 - val_accuracy: 0.8889\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.2445 - val_accuracy: 0.8819\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9992 - val_loss: 0.2493 - val_accuracy: 0.8889\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.2451 - val_accuracy: 0.8889\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.2453 - val_accuracy: 0.8889\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0243 - accuracy: 0.9915 - val_loss: 0.2467 - val_accuracy: 0.8958\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0296 - accuracy: 0.9931 - val_loss: 0.2434 - val_accuracy: 0.8889\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.8889\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.2444 - val_accuracy: 0.8889\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.8889\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0242 - accuracy: 0.9938 - val_loss: 0.2445 - val_accuracy: 0.8819\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.2453 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.2445 - val_accuracy: 0.8889\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.2477 - val_accuracy: 0.8889\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.2480 - val_accuracy: 0.8889\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.2455 - val_accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0143 - accuracy: 0.9938 - val_loss: 0.2438 - val_accuracy: 0.8819\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.2462 - val_accuracy: 0.8889\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 18s 110ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.2455 - val_accuracy: 0.8819\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0118 - accuracy: 0.9985 - val_loss: 0.2453 - val_accuracy: 0.8819\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.2454 - val_accuracy: 0.8819\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.2467 - val_accuracy: 0.8819\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.2494 - val_accuracy: 0.8958\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.2463 - val_accuracy: 0.8819\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.2453 - val_accuracy: 0.8819\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0121 - accuracy: 0.9992 - val_loss: 0.2456 - val_accuracy: 0.8819\n",
      "Score for fold 4: loss of 0.24562504887580872; accuracy of 88.19444179534912%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 116ms/step - loss: 0.3825 - accuracy: 0.7153 - val_loss: 4.0109 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2969 - accuracy: 0.8063 - val_loss: 2.0139 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2449 - accuracy: 0.8480 - val_loss: 0.3760 - val_accuracy: 0.7569\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.2069 - accuracy: 0.8742 - val_loss: 0.2808 - val_accuracy: 0.8264\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1714 - accuracy: 0.8997 - val_loss: 0.1929 - val_accuracy: 0.8819\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.1203 - accuracy: 0.9290 - val_loss: 0.2626 - val_accuracy: 0.8472\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1137 - accuracy: 0.9421 - val_loss: 0.2654 - val_accuracy: 0.8542\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1022 - accuracy: 0.9468 - val_loss: 0.3191 - val_accuracy: 0.8681\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0843 - accuracy: 0.9576 - val_loss: 0.3199 - val_accuracy: 0.8264\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0645 - accuracy: 0.9738 - val_loss: 0.6343 - val_accuracy: 0.7708\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0606 - accuracy: 0.9807 - val_loss: 0.3590 - val_accuracy: 0.8194\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0673 - accuracy: 0.9622 - val_loss: 0.4229 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0425 - accuracy: 0.9838 - val_loss: 0.3175 - val_accuracy: 0.8681\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0392 - accuracy: 0.9846 - val_loss: 0.1857 - val_accuracy: 0.8750\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0427 - accuracy: 0.9823 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0263 - accuracy: 0.9954 - val_loss: 0.2309 - val_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 18s 110ms/step - loss: 0.0370 - accuracy: 0.9900 - val_loss: 0.3703 - val_accuracy: 0.8472\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 108ms/step - loss: 0.0241 - accuracy: 0.9954 - val_loss: 0.2318 - val_accuracy: 0.8889\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0271 - accuracy: 0.9892 - val_loss: 0.2138 - val_accuracy: 0.8889\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0188 - accuracy: 0.9946 - val_loss: 0.2103 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.2072 - val_accuracy: 0.8750\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.2012 - val_accuracy: 0.8819\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.2011 - val_accuracy: 0.8819\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0136 - accuracy: 0.9961 - val_loss: 0.2031 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0134 - accuracy: 0.9985 - val_loss: 0.2012 - val_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0266 - accuracy: 0.9907 - val_loss: 0.2044 - val_accuracy: 0.8750\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.1992 - val_accuracy: 0.8889\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 0.2076 - val_accuracy: 0.8681\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0225 - accuracy: 0.9892 - val_loss: 0.2014 - val_accuracy: 0.8750\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.2014 - val_accuracy: 0.8819\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0140 - accuracy: 0.9977 - val_loss: 0.1970 - val_accuracy: 0.8750\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.1967 - val_accuracy: 0.8681\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0088 - accuracy: 0.9992 - val_loss: 0.1970 - val_accuracy: 0.8750\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0259 - accuracy: 0.9915 - val_loss: 0.1986 - val_accuracy: 0.8681\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.1959 - val_accuracy: 0.8681\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.1970 - val_accuracy: 0.8681\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.8681\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.1991 - val_accuracy: 0.8681\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0146 - accuracy: 0.9961 - val_loss: 0.1981 - val_accuracy: 0.8681\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2007 - val_accuracy: 0.8681\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.2006 - val_accuracy: 0.8750\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.1996 - val_accuracy: 0.8750\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1992 - val_accuracy: 0.8681\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.2003 - val_accuracy: 0.8681\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.1987 - val_accuracy: 0.8681\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.1995 - val_accuracy: 0.8681\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.1988 - val_accuracy: 0.8681\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.2016 - val_accuracy: 0.8681\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.1995 - val_accuracy: 0.8750\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.8681\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.8681\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2000 - val_accuracy: 0.8750\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0079 - accuracy: 0.9992 - val_loss: 0.2009 - val_accuracy: 0.8681\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.2009 - val_accuracy: 0.8681\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.1991 - val_accuracy: 0.8681\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.2005 - val_accuracy: 0.8681\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.8681\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.2015 - val_accuracy: 0.8681\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.1997 - val_accuracy: 0.8681\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.2017 - val_accuracy: 0.8681\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.2011 - val_accuracy: 0.8681\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.2019 - val_accuracy: 0.8750\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.2007 - val_accuracy: 0.8611\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.2004 - val_accuracy: 0.8681\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.2019 - val_accuracy: 0.8681\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0084 - accuracy: 0.9992 - val_loss: 0.2010 - val_accuracy: 0.8611\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0077 - accuracy: 0.9992 - val_loss: 0.2004 - val_accuracy: 0.8750\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.2012 - val_accuracy: 0.8681\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.2002 - val_accuracy: 0.8681\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0129 - accuracy: 0.9992 - val_loss: 0.1995 - val_accuracy: 0.8681\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.1999 - val_accuracy: 0.8681\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0089 - accuracy: 0.9992 - val_loss: 0.2004 - val_accuracy: 0.8611\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.2008 - val_accuracy: 0.8681\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.2005 - val_accuracy: 0.8681\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0126 - accuracy: 0.9977 - val_loss: 0.1997 - val_accuracy: 0.8681\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0090 - accuracy: 0.9992 - val_loss: 0.2002 - val_accuracy: 0.8681\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.2017 - val_accuracy: 0.8750\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.2012 - val_accuracy: 0.8611\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.2016 - val_accuracy: 0.8819\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.2003 - val_accuracy: 0.8681\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.2012 - val_accuracy: 0.8611\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.2004 - val_accuracy: 0.8611\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.2012 - val_accuracy: 0.8611\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.1982 - val_accuracy: 0.8681\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 0.2010 - val_accuracy: 0.8681\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.2004 - val_accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0148 - accuracy: 0.9977 - val_loss: 0.2003 - val_accuracy: 0.8681\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.2000 - val_accuracy: 0.8681\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0206 - accuracy: 0.9938 - val_loss: 0.1997 - val_accuracy: 0.8681\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.1992 - val_accuracy: 0.8681\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.2008 - val_accuracy: 0.8681\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0142 - accuracy: 0.9977 - val_loss: 0.1994 - val_accuracy: 0.8681\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.2009 - val_accuracy: 0.8681\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0080 - accuracy: 0.9992 - val_loss: 0.1994 - val_accuracy: 0.8681\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.2006 - val_accuracy: 0.8681\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.2000 - val_accuracy: 0.8681\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0124 - accuracy: 0.9946 - val_loss: 0.1993 - val_accuracy: 0.8681\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0139 - accuracy: 0.9977 - val_loss: 0.1993 - val_accuracy: 0.8681\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.1994 - val_accuracy: 0.8681\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.2006 - val_accuracy: 0.8681\n",
      "Score for fold 5: loss of 0.20063143968582153; accuracy of 86.80555820465088%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 116ms/step - loss: 0.3732 - accuracy: 0.7431 - val_loss: 1.4048 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2769 - accuracy: 0.8279 - val_loss: 2.2061 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2415 - accuracy: 0.8488 - val_loss: 0.5615 - val_accuracy: 0.6736\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2242 - accuracy: 0.8495 - val_loss: 0.2668 - val_accuracy: 0.8264\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1968 - accuracy: 0.8889 - val_loss: 2.5541 - val_accuracy: 0.5556\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1768 - accuracy: 0.8904 - val_loss: 0.2062 - val_accuracy: 0.8750\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1402 - accuracy: 0.9198 - val_loss: 0.6729 - val_accuracy: 0.7292\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.1203 - accuracy: 0.9383 - val_loss: 0.2498 - val_accuracy: 0.9097\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1106 - accuracy: 0.9437 - val_loss: 0.3403 - val_accuracy: 0.8056\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0917 - accuracy: 0.9506 - val_loss: 0.2250 - val_accuracy: 0.8889\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0857 - accuracy: 0.9576 - val_loss: 0.5915 - val_accuracy: 0.7708\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0643 - accuracy: 0.9722 - val_loss: 0.2906 - val_accuracy: 0.8542\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0711 - accuracy: 0.9668 - val_loss: 0.3016 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0597 - accuracy: 0.9668 - val_loss: 0.2775 - val_accuracy: 0.8750\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0447 - accuracy: 0.9823 - val_loss: 0.3593 - val_accuracy: 0.8403\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0416 - accuracy: 0.9838 - val_loss: 0.1863 - val_accuracy: 0.8958\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0253 - accuracy: 0.9907 - val_loss: 0.2658 - val_accuracy: 0.8750\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0374 - accuracy: 0.9815 - val_loss: 0.1798 - val_accuracy: 0.9097\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0343 - accuracy: 0.9853 - val_loss: 0.4302 - val_accuracy: 0.8403\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0279 - accuracy: 0.9884 - val_loss: 0.2153 - val_accuracy: 0.9028\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 0.1779 - val_accuracy: 0.9167\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.1771 - val_accuracy: 0.8889\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0149 - accuracy: 0.9985 - val_loss: 0.1838 - val_accuracy: 0.8889\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.1802 - val_accuracy: 0.8819\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.1869 - val_accuracy: 0.8819\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.1787 - val_accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0177 - accuracy: 0.9961 - val_loss: 0.1982 - val_accuracy: 0.8819\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0161 - accuracy: 0.9961 - val_loss: 0.1954 - val_accuracy: 0.8819\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0180 - accuracy: 0.9931 - val_loss: 0.1988 - val_accuracy: 0.8819\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.1812 - val_accuracy: 0.8889\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0147 - accuracy: 0.9977 - val_loss: 0.1795 - val_accuracy: 0.8889\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0287 - accuracy: 0.9915 - val_loss: 0.1739 - val_accuracy: 0.8889\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.1735 - val_accuracy: 0.8889\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.1799 - val_accuracy: 0.8889\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.1773 - val_accuracy: 0.8958\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0147 - accuracy: 0.9977 - val_loss: 0.1767 - val_accuracy: 0.9028\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.1811 - val_accuracy: 0.8958\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.1817 - val_accuracy: 0.8958\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.1849 - val_accuracy: 0.8889\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.1801 - val_accuracy: 0.9028\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0128 - accuracy: 0.9985 - val_loss: 0.1803 - val_accuracy: 0.8958\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.1820 - val_accuracy: 0.8889\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.1822 - val_accuracy: 0.8889\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.1812 - val_accuracy: 0.8889\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.1808 - val_accuracy: 0.8889\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.1805 - val_accuracy: 0.8958\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.1824 - val_accuracy: 0.8889\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0097 - accuracy: 0.9992 - val_loss: 0.1813 - val_accuracy: 0.8889\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0141 - accuracy: 0.9977 - val_loss: 0.1808 - val_accuracy: 0.8889\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.1823 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 0.1811 - val_accuracy: 0.8889\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.1819 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.1802 - val_accuracy: 0.8958\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.1799 - val_accuracy: 0.8889\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0187 - accuracy: 0.9931 - val_loss: 0.1794 - val_accuracy: 0.8889\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0088 - accuracy: 0.9992 - val_loss: 0.1809 - val_accuracy: 0.8958\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0130 - accuracy: 0.9992 - val_loss: 0.1827 - val_accuracy: 0.8889\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.1829 - val_accuracy: 0.8889\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.1821 - val_accuracy: 0.8889\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 0.1824 - val_accuracy: 0.8889\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.1800 - val_accuracy: 0.8958\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1844 - val_accuracy: 0.8889\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1808 - val_accuracy: 0.8958\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.1801 - val_accuracy: 0.8889\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.1841 - val_accuracy: 0.8889\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0246 - accuracy: 0.9915 - val_loss: 0.1819 - val_accuracy: 0.8958\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.1793 - val_accuracy: 0.8958\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.1791 - val_accuracy: 0.8958\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 0.1787 - val_accuracy: 0.9028\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.1802 - val_accuracy: 0.8889\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.1784 - val_accuracy: 0.8958\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0140 - accuracy: 0.9992 - val_loss: 0.1800 - val_accuracy: 0.8958\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.1796 - val_accuracy: 0.8958\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.1829 - val_accuracy: 0.8958\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0108 - accuracy: 0.9992 - val_loss: 0.1848 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1813 - val_accuracy: 0.8889\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.1818 - val_accuracy: 0.8958\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0213 - accuracy: 0.9938 - val_loss: 0.1817 - val_accuracy: 0.8958\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1812 - val_accuracy: 0.8958\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1805 - val_accuracy: 0.8958\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.1800 - val_accuracy: 0.8958\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.1811 - val_accuracy: 0.8958\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 0.1803 - val_accuracy: 0.8958\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.1815 - val_accuracy: 0.8958\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1833 - val_accuracy: 0.8889\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.1810 - val_accuracy: 0.8958\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.1809 - val_accuracy: 0.8958\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 0.1809 - val_accuracy: 0.8958\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.1799 - val_accuracy: 0.8958\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0137 - accuracy: 0.9977 - val_loss: 0.1780 - val_accuracy: 0.8958\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.8958\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.1818 - val_accuracy: 0.8958\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0109 - accuracy: 0.9992 - val_loss: 0.1799 - val_accuracy: 0.9028\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0102 - accuracy: 0.9992 - val_loss: 0.1806 - val_accuracy: 0.8958\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.8958\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0155 - accuracy: 0.9977 - val_loss: 0.1821 - val_accuracy: 0.8889\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0096 - accuracy: 0.9992 - val_loss: 0.1803 - val_accuracy: 0.8958\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 0.1781 - val_accuracy: 0.8958\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1777 - val_accuracy: 0.8958\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.1777 - val_accuracy: 0.8958\n",
      "Score for fold 6: loss of 0.1776886284351349; accuracy of 89.58333134651184%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 117ms/step - loss: 0.3692 - accuracy: 0.7323 - val_loss: 2.7900 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.3069 - accuracy: 0.7978 - val_loss: 3.6272 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2706 - accuracy: 0.8349 - val_loss: 0.4728 - val_accuracy: 0.6944\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.2195 - accuracy: 0.8642 - val_loss: 0.2927 - val_accuracy: 0.7778\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1923 - accuracy: 0.8927 - val_loss: 0.4304 - val_accuracy: 0.7222\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1680 - accuracy: 0.9035 - val_loss: 0.4221 - val_accuracy: 0.7431\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1144 - accuracy: 0.9483 - val_loss: 0.1950 - val_accuracy: 0.9028\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.1226 - accuracy: 0.9360 - val_loss: 0.7231 - val_accuracy: 0.6944\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1099 - accuracy: 0.9437 - val_loss: 0.4180 - val_accuracy: 0.8056\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0940 - accuracy: 0.9483 - val_loss: 0.4330 - val_accuracy: 0.8194\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0731 - accuracy: 0.9684 - val_loss: 0.5683 - val_accuracy: 0.8125\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0385 - accuracy: 0.9861 - val_loss: 0.2674 - val_accuracy: 0.8958\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 18s 110ms/step - loss: 0.0510 - accuracy: 0.9776 - val_loss: 0.3085 - val_accuracy: 0.8958\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 18s 110ms/step - loss: 0.0444 - accuracy: 0.9807 - val_loss: 0.2404 - val_accuracy: 0.8958\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0525 - accuracy: 0.9730 - val_loss: 0.1632 - val_accuracy: 0.9028\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0286 - accuracy: 0.9892 - val_loss: 0.2488 - val_accuracy: 0.9028\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0343 - accuracy: 0.9846 - val_loss: 0.5587 - val_accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0301 - accuracy: 0.9884 - val_loss: 0.2927 - val_accuracy: 0.8819\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.4230 - val_accuracy: 0.8264\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0305 - accuracy: 0.9931 - val_loss: 0.2078 - val_accuracy: 0.9028\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.2006 - val_accuracy: 0.9236\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.2046 - val_accuracy: 0.9236\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.2126 - val_accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0199 - accuracy: 0.9969 - val_loss: 0.2062 - val_accuracy: 0.9028\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0159 - accuracy: 0.9961 - val_loss: 0.2273 - val_accuracy: 0.9028\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.2156 - val_accuracy: 0.9028\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.2101 - val_accuracy: 0.9097\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0176 - accuracy: 0.9931 - val_loss: 0.2312 - val_accuracy: 0.8889\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.2204 - val_accuracy: 0.9028\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0180 - accuracy: 0.9946 - val_loss: 0.2079 - val_accuracy: 0.9097\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.9097\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.2191 - val_accuracy: 0.8958\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.2165 - val_accuracy: 0.8958\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0171 - accuracy: 0.9969 - val_loss: 0.2116 - val_accuracy: 0.8889\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.2146 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0173 - accuracy: 0.9931 - val_loss: 0.2111 - val_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.2171 - val_accuracy: 0.8819\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.2142 - val_accuracy: 0.9028\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.2062 - val_accuracy: 0.9097\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.2150 - val_accuracy: 0.8889\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.2153 - val_accuracy: 0.8889\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0106 - accuracy: 0.9992 - val_loss: 0.2129 - val_accuracy: 0.9028\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0139 - accuracy: 0.9961 - val_loss: 0.2197 - val_accuracy: 0.8958\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.2162 - val_accuracy: 0.8889\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.2155 - val_accuracy: 0.8889\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.2137 - val_accuracy: 0.8889\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0131 - accuracy: 0.9977 - val_loss: 0.2114 - val_accuracy: 0.8958\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.2113 - val_accuracy: 0.8958\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.2134 - val_accuracy: 0.8889\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0286 - accuracy: 0.9892 - val_loss: 0.2144 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0104 - accuracy: 0.9992 - val_loss: 0.2127 - val_accuracy: 0.8958\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0165 - accuracy: 0.9977 - val_loss: 0.2157 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.2124 - val_accuracy: 0.8958\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.2139 - val_accuracy: 0.8819\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.2175 - val_accuracy: 0.8958\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.2159 - val_accuracy: 0.8889\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.2160 - val_accuracy: 0.8889\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 0.2132 - val_accuracy: 0.8958\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.2169 - val_accuracy: 0.8958\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.2150 - val_accuracy: 0.8819\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0076 - accuracy: 0.9992 - val_loss: 0.2134 - val_accuracy: 0.8819\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.2183 - val_accuracy: 0.8889\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 0.2137 - val_accuracy: 0.8958\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.2140 - val_accuracy: 0.8958\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0168 - accuracy: 0.9961 - val_loss: 0.2154 - val_accuracy: 0.8889\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.2151 - val_accuracy: 0.8958\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.2119 - val_accuracy: 0.8958\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.2163 - val_accuracy: 0.8819\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 0.2173 - val_accuracy: 0.8958\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.2130 - val_accuracy: 0.9028\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.2160 - val_accuracy: 0.8819\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.2179 - val_accuracy: 0.8958\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.2195 - val_accuracy: 0.8958\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 0.2158 - val_accuracy: 0.8889\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0118 - accuracy: 0.9977 - val_loss: 0.2175 - val_accuracy: 0.8958\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2128 - val_accuracy: 0.8958\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0173 - accuracy: 0.9900 - val_loss: 0.2142 - val_accuracy: 0.8819\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0101 - accuracy: 0.9961 - val_loss: 0.2146 - val_accuracy: 0.8819\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.2180 - val_accuracy: 0.8958\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0161 - accuracy: 0.9977 - val_loss: 0.2175 - val_accuracy: 0.8889\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.2161 - val_accuracy: 0.8819\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.2197 - val_accuracy: 0.8958\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.2197 - val_accuracy: 0.8958\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.2176 - val_accuracy: 0.8889\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.2139 - val_accuracy: 0.8889\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2138 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.2139 - val_accuracy: 0.8819\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0156 - accuracy: 0.9985 - val_loss: 0.2148 - val_accuracy: 0.8819\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.8889\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.2153 - val_accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0100 - accuracy: 0.9992 - val_loss: 0.2249 - val_accuracy: 0.8958\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.2198 - val_accuracy: 0.8958\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.8819\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.2174 - val_accuracy: 0.8958\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.2168 - val_accuracy: 0.8819\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.2162 - val_accuracy: 0.8819\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0229 - accuracy: 0.9946 - val_loss: 0.2199 - val_accuracy: 0.8958\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0119 - accuracy: 0.9977 - val_loss: 0.2182 - val_accuracy: 0.8958\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9992 - val_loss: 0.2198 - val_accuracy: 0.8958\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.2140 - val_accuracy: 0.8889\n",
      "Score for fold 7: loss of 0.2139722853899002; accuracy of 88.88888955116272%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 115ms/step - loss: 0.3800 - accuracy: 0.7299 - val_loss: 2.4926 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.3109 - accuracy: 0.7832 - val_loss: 3.2616 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2585 - accuracy: 0.8387 - val_loss: 0.2359 - val_accuracy: 0.8472\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2201 - accuracy: 0.8665 - val_loss: 0.2381 - val_accuracy: 0.8542\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1910 - accuracy: 0.8819 - val_loss: 0.3531 - val_accuracy: 0.8194\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 19s 116ms/step - loss: 0.1581 - accuracy: 0.9082 - val_loss: 0.8259 - val_accuracy: 0.7014\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 18s 112ms/step - loss: 0.1534 - accuracy: 0.9174 - val_loss: 0.1486 - val_accuracy: 0.9167\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.1066 - accuracy: 0.9437 - val_loss: 0.2769 - val_accuracy: 0.8750\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1202 - accuracy: 0.9344 - val_loss: 0.2252 - val_accuracy: 0.8681\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0957 - accuracy: 0.9475 - val_loss: 0.2106 - val_accuracy: 0.9167\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0795 - accuracy: 0.9545 - val_loss: 0.5194 - val_accuracy: 0.7847\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0849 - accuracy: 0.9568 - val_loss: 0.1960 - val_accuracy: 0.9028\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0568 - accuracy: 0.9753 - val_loss: 0.2367 - val_accuracy: 0.8611\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0553 - accuracy: 0.9769 - val_loss: 0.2121 - val_accuracy: 0.9028\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0268 - accuracy: 0.9954 - val_loss: 0.2258 - val_accuracy: 0.9097\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0457 - accuracy: 0.9807 - val_loss: 0.3993 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0307 - accuracy: 0.9877 - val_loss: 0.5360 - val_accuracy: 0.7917\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0496 - accuracy: 0.9769 - val_loss: 0.4003 - val_accuracy: 0.8403\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0243 - accuracy: 0.9892 - val_loss: 0.2925 - val_accuracy: 0.8542\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0439 - accuracy: 0.9776 - val_loss: 0.1327 - val_accuracy: 0.9375\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0310 - accuracy: 0.9907 - val_loss: 0.1457 - val_accuracy: 0.9375\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.1548 - val_accuracy: 0.9444\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0158 - accuracy: 0.9938 - val_loss: 0.1520 - val_accuracy: 0.9306\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0173 - accuracy: 0.9938 - val_loss: 0.1588 - val_accuracy: 0.9306\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 0.1436 - val_accuracy: 0.9236\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1398 - val_accuracy: 0.9306\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.1590 - val_accuracy: 0.9375\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.1478 - val_accuracy: 0.9375\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1604 - val_accuracy: 0.9236\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9992 - val_loss: 0.1615 - val_accuracy: 0.9236\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0145 - accuracy: 0.9961 - val_loss: 0.1607 - val_accuracy: 0.9236\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0122 - accuracy: 0.9992 - val_loss: 0.1497 - val_accuracy: 0.9306\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.1619 - val_accuracy: 0.9306\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1640 - val_accuracy: 0.9236\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0142 - accuracy: 0.9961 - val_loss: 0.1754 - val_accuracy: 0.9306\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 0.1636 - val_accuracy: 0.9236\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.1775 - val_accuracy: 0.9306\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.1567 - val_accuracy: 0.9236\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 0.1540 - val_accuracy: 0.9306\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0117 - accuracy: 0.9992 - val_loss: 0.1553 - val_accuracy: 0.9306\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.1602 - val_accuracy: 0.9236\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0153 - accuracy: 0.9977 - val_loss: 0.1622 - val_accuracy: 0.9236\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1591 - val_accuracy: 0.9236\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0090 - accuracy: 0.9992 - val_loss: 0.1563 - val_accuracy: 0.9236\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0164 - accuracy: 0.9961 - val_loss: 0.1602 - val_accuracy: 0.9236\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9236\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9992 - val_loss: 0.1578 - val_accuracy: 0.9306\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0110 - accuracy: 0.9992 - val_loss: 0.1568 - val_accuracy: 0.9236\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0153 - accuracy: 0.9977 - val_loss: 0.1580 - val_accuracy: 0.9236\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.1599 - val_accuracy: 0.9236\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.1597 - val_accuracy: 0.9236\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0101 - accuracy: 0.9992 - val_loss: 0.1545 - val_accuracy: 0.9306\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9306\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.1604 - val_accuracy: 0.9236\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0151 - accuracy: 0.9931 - val_loss: 0.1572 - val_accuracy: 0.9236\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 0.1560 - val_accuracy: 0.9375\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1567 - val_accuracy: 0.9306\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0108 - accuracy: 0.9992 - val_loss: 0.1568 - val_accuracy: 0.9236\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1557 - val_accuracy: 0.9306\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9236\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.1567 - val_accuracy: 0.9306\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.1581 - val_accuracy: 0.9236\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.1625 - val_accuracy: 0.9236\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0171 - accuracy: 0.9938 - val_loss: 0.1599 - val_accuracy: 0.9236\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.1557 - val_accuracy: 0.9306\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.1582 - val_accuracy: 0.9236\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0224 - accuracy: 0.9954 - val_loss: 0.1575 - val_accuracy: 0.9236\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.1588 - val_accuracy: 0.9236\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0181 - accuracy: 0.9961 - val_loss: 0.1619 - val_accuracy: 0.9236\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.1584 - val_accuracy: 0.9236\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9236\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0087 - accuracy: 0.9992 - val_loss: 0.1610 - val_accuracy: 0.9236\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0142 - accuracy: 0.9961 - val_loss: 0.1598 - val_accuracy: 0.9236\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.1564 - val_accuracy: 0.9306\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.1605 - val_accuracy: 0.9236\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9306\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9375\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9375\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0113 - accuracy: 0.9992 - val_loss: 0.1603 - val_accuracy: 0.9236\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 0.1600 - val_accuracy: 0.9236\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0128 - accuracy: 0.9977 - val_loss: 0.1588 - val_accuracy: 0.9375\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.1555 - val_accuracy: 0.9375\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9992 - val_loss: 0.1588 - val_accuracy: 0.9236\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 0.1639 - val_accuracy: 0.9236\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0138 - accuracy: 0.9992 - val_loss: 0.1620 - val_accuracy: 0.9236\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9236\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0206 - accuracy: 0.9946 - val_loss: 0.1628 - val_accuracy: 0.9236\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0123 - accuracy: 0.9992 - val_loss: 0.1588 - val_accuracy: 0.9236\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.1592 - val_accuracy: 0.9236\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.1591 - val_accuracy: 0.9236\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0221 - accuracy: 0.9946 - val_loss: 0.1582 - val_accuracy: 0.9236\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0100 - accuracy: 0.9992 - val_loss: 0.1582 - val_accuracy: 0.9236\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9977 - val_loss: 0.1615 - val_accuracy: 0.9236\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.1632 - val_accuracy: 0.9236\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.1592 - val_accuracy: 0.9306\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1595 - val_accuracy: 0.9236\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0119 - accuracy: 0.9992 - val_loss: 0.1591 - val_accuracy: 0.9236\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0141 - accuracy: 0.9992 - val_loss: 0.1600 - val_accuracy: 0.9236\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0143 - accuracy: 0.9977 - val_loss: 0.1605 - val_accuracy: 0.9236\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.1620 - val_accuracy: 0.9236\n",
      "Score for fold 8: loss of 0.16198259592056274; accuracy of 92.36111044883728%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 26s 112ms/step - loss: 0.3800 - accuracy: 0.7384 - val_loss: 1.8835 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.3127 - accuracy: 0.8002 - val_loss: 3.9844 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.2461 - accuracy: 0.8457 - val_loss: 0.5953 - val_accuracy: 0.6250\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.2300 - accuracy: 0.8395 - val_loss: 0.2750 - val_accuracy: 0.8056\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.1935 - accuracy: 0.8781 - val_loss: 0.3172 - val_accuracy: 0.7917\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.1424 - accuracy: 0.9228 - val_loss: 0.4747 - val_accuracy: 0.7847\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.1233 - accuracy: 0.9329 - val_loss: 0.3330 - val_accuracy: 0.7986\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.1099 - accuracy: 0.9429 - val_loss: 0.2635 - val_accuracy: 0.8819\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.1032 - accuracy: 0.9475 - val_loss: 0.3447 - val_accuracy: 0.8264\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0706 - accuracy: 0.9676 - val_loss: 0.4133 - val_accuracy: 0.8125\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0751 - accuracy: 0.9606 - val_loss: 0.5348 - val_accuracy: 0.7708\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0531 - accuracy: 0.9799 - val_loss: 0.2988 - val_accuracy: 0.8056\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0587 - accuracy: 0.9684 - val_loss: 0.4481 - val_accuracy: 0.8333\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0432 - accuracy: 0.9853 - val_loss: 0.4289 - val_accuracy: 0.7986\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0393 - accuracy: 0.9823 - val_loss: 0.2847 - val_accuracy: 0.8472\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.2576 - val_accuracy: 0.8264\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0291 - accuracy: 0.9869 - val_loss: 0.3108 - val_accuracy: 0.8681\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0231 - accuracy: 0.9954 - val_loss: 0.3213 - val_accuracy: 0.8611\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0364 - accuracy: 0.9838 - val_loss: 0.2960 - val_accuracy: 0.8750\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.4887 - val_accuracy: 0.8056\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0203 - accuracy: 0.9907 - val_loss: 0.2863 - val_accuracy: 0.8542\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.2751 - val_accuracy: 0.8611\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0292 - accuracy: 0.9900 - val_loss: 0.2852 - val_accuracy: 0.8472\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 0.2847 - val_accuracy: 0.8611\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.2876 - val_accuracy: 0.8611\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0125 - accuracy: 0.9985 - val_loss: 0.2684 - val_accuracy: 0.8611\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.2708 - val_accuracy: 0.8542\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.2748 - val_accuracy: 0.8611\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0135 - accuracy: 0.9969 - val_loss: 0.2841 - val_accuracy: 0.8611\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0128 - accuracy: 0.9985 - val_loss: 0.2914 - val_accuracy: 0.8542\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0209 - accuracy: 0.9969 - val_loss: 0.2852 - val_accuracy: 0.8611\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2827 - val_accuracy: 0.8472\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.2786 - val_accuracy: 0.8542\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.2693 - val_accuracy: 0.8542\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.2745 - val_accuracy: 0.8611\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0162 - accuracy: 0.9969 - val_loss: 0.2880 - val_accuracy: 0.8611\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0248 - accuracy: 0.9954 - val_loss: 0.2815 - val_accuracy: 0.8611\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.2828 - val_accuracy: 0.8542\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0191 - accuracy: 0.9931 - val_loss: 0.2774 - val_accuracy: 0.8542\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0112 - accuracy: 0.9992 - val_loss: 0.2773 - val_accuracy: 0.8611\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.2785 - val_accuracy: 0.8611\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.2763 - val_accuracy: 0.8542\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0111 - accuracy: 0.9992 - val_loss: 0.2762 - val_accuracy: 0.8611\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 0.2783 - val_accuracy: 0.8611\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 16s 101ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.8611\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0099 - accuracy: 0.9992 - val_loss: 0.2767 - val_accuracy: 0.8611\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.2780 - val_accuracy: 0.8542\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2784 - val_accuracy: 0.8611\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.2779 - val_accuracy: 0.8542\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.8611\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0094 - accuracy: 0.9992 - val_loss: 0.2774 - val_accuracy: 0.8611\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2767 - val_accuracy: 0.8542\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.2773 - val_accuracy: 0.8542\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2780 - val_accuracy: 0.8542\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.2779 - val_accuracy: 0.8542\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.2789 - val_accuracy: 0.8542\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.8542\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0093 - accuracy: 0.9961 - val_loss: 0.2777 - val_accuracy: 0.8542\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0190 - accuracy: 0.9931 - val_loss: 0.2795 - val_accuracy: 0.8542\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.2760 - val_accuracy: 0.8542\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0118 - accuracy: 0.9977 - val_loss: 0.2779 - val_accuracy: 0.8542\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.2785 - val_accuracy: 0.8542\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.2788 - val_accuracy: 0.8542\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 0.2789 - val_accuracy: 0.8542\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.2779 - val_accuracy: 0.8542\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 18s 111ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0140 - accuracy: 0.9977 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0105 - accuracy: 0.9992 - val_loss: 0.2804 - val_accuracy: 0.8611\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.2845 - val_accuracy: 0.8542\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.2799 - val_accuracy: 0.8542\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.2802 - val_accuracy: 0.8542\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 104ms/step - loss: 0.0216 - accuracy: 0.9915 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.8542\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0202 - accuracy: 0.9946 - val_loss: 0.2796 - val_accuracy: 0.8542\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.2807 - val_accuracy: 0.8611\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 18s 111ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.2808 - val_accuracy: 0.8542\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0094 - accuracy: 0.9992 - val_loss: 0.2807 - val_accuracy: 0.8542\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 0.2797 - val_accuracy: 0.8542\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0153 - accuracy: 0.9961 - val_loss: 0.2819 - val_accuracy: 0.8542\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0114 - accuracy: 0.9992 - val_loss: 0.2826 - val_accuracy: 0.8542\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.2788 - val_accuracy: 0.8542\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.2785 - val_accuracy: 0.8542\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0168 - accuracy: 0.9961 - val_loss: 0.2806 - val_accuracy: 0.8542\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 16s 101ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.2783 - val_accuracy: 0.8542\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 103ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2784 - val_accuracy: 0.8472\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.2805 - val_accuracy: 0.8611\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.2785 - val_accuracy: 0.8542\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0122 - accuracy: 0.9992 - val_loss: 0.2804 - val_accuracy: 0.8542\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.2816 - val_accuracy: 0.8542\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 0.2818 - val_accuracy: 0.8542\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.2793 - val_accuracy: 0.8542\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.2765 - val_accuracy: 0.8542\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0238 - accuracy: 0.9938 - val_loss: 0.2802 - val_accuracy: 0.8472\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.2794 - val_accuracy: 0.8542\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 16s 101ms/step - loss: 0.0076 - accuracy: 0.9992 - val_loss: 0.2779 - val_accuracy: 0.8542\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 16s 102ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.2805 - val_accuracy: 0.8542\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 102ms/step - loss: 0.0274 - accuracy: 0.9907 - val_loss: 0.2809 - val_accuracy: 0.8542\n",
      "Score for fold 9: loss of 0.28090229630470276; accuracy of 85.41666865348816%\n",
      "(1296,) (144,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 27s 115ms/step - loss: 0.3740 - accuracy: 0.7361 - val_loss: 2.2439 - val_accuracy: 0.3403\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2875 - accuracy: 0.8025 - val_loss: 1.9323 - val_accuracy: 0.3403\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.2252 - accuracy: 0.8619 - val_loss: 0.4497 - val_accuracy: 0.7222\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1913 - accuracy: 0.8881 - val_loss: 0.4930 - val_accuracy: 0.7431\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1597 - accuracy: 0.9066 - val_loss: 0.5013 - val_accuracy: 0.7778\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1353 - accuracy: 0.9221 - val_loss: 0.2609 - val_accuracy: 0.8681\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.1345 - accuracy: 0.9313 - val_loss: 0.2847 - val_accuracy: 0.8542\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0986 - accuracy: 0.9506 - val_loss: 0.4606 - val_accuracy: 0.7778\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.1011 - accuracy: 0.9491 - val_loss: 0.7742 - val_accuracy: 0.7153\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0662 - accuracy: 0.9753 - val_loss: 0.1654 - val_accuracy: 0.8958\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0490 - accuracy: 0.9807 - val_loss: 0.1828 - val_accuracy: 0.8819\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0607 - accuracy: 0.9738 - val_loss: 0.2573 - val_accuracy: 0.8542\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0593 - accuracy: 0.9707 - val_loss: 0.3106 - val_accuracy: 0.8403\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.1738 - val_accuracy: 0.9097\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0317 - accuracy: 0.9869 - val_loss: 0.2150 - val_accuracy: 0.8750\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0309 - accuracy: 0.9869 - val_loss: 0.1908 - val_accuracy: 0.8958\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0304 - accuracy: 0.9884 - val_loss: 0.2188 - val_accuracy: 0.8750\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.2471 - val_accuracy: 0.8819\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0328 - accuracy: 0.9877 - val_loss: 0.6308 - val_accuracy: 0.8125\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0347 - accuracy: 0.9869 - val_loss: 0.1646 - val_accuracy: 0.9097\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.1555 - val_accuracy: 0.9028\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.1545 - val_accuracy: 0.9236\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 0.1527 - val_accuracy: 0.9097\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.1512 - val_accuracy: 0.9167\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.1467 - val_accuracy: 0.9236\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0120 - accuracy: 0.9992 - val_loss: 0.1484 - val_accuracy: 0.9236\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0250 - accuracy: 0.9931 - val_loss: 0.1476 - val_accuracy: 0.9167\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0140 - accuracy: 0.9992 - val_loss: 0.1490 - val_accuracy: 0.9167\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 0.1451 - val_accuracy: 0.9167\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.1459 - val_accuracy: 0.9306\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.1498 - val_accuracy: 0.9236\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0208 - accuracy: 0.9961 - val_loss: 0.1485 - val_accuracy: 0.9236\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.1493 - val_accuracy: 0.9097\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.1514 - val_accuracy: 0.9097\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0110 - accuracy: 0.9992 - val_loss: 0.1479 - val_accuracy: 0.9167\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.1444 - val_accuracy: 0.9097\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0154 - accuracy: 0.9977 - val_loss: 0.1489 - val_accuracy: 0.9167\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.1455 - val_accuracy: 0.9167\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9097\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.1454 - val_accuracy: 0.9167\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.1453 - val_accuracy: 0.9167\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.1454 - val_accuracy: 0.9167\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.1461 - val_accuracy: 0.9167\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0106 - accuracy: 0.9992 - val_loss: 0.1459 - val_accuracy: 0.9097\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.1453 - val_accuracy: 0.9097\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0112 - accuracy: 0.9992 - val_loss: 0.1460 - val_accuracy: 0.9167\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0079 - accuracy: 0.9992 - val_loss: 0.1447 - val_accuracy: 0.9167\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.1455 - val_accuracy: 0.9167\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9167\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1453 - val_accuracy: 0.9097\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1450 - val_accuracy: 0.9097\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0089 - accuracy: 0.9992 - val_loss: 0.1449 - val_accuracy: 0.9097\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.1452 - val_accuracy: 0.9097\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.1445 - val_accuracy: 0.9236\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.1444 - val_accuracy: 0.9097\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.1446 - val_accuracy: 0.9167\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.1440 - val_accuracy: 0.9097\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 0.9097\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.1438 - val_accuracy: 0.9097\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9097\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.1448 - val_accuracy: 0.9097\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.1446 - val_accuracy: 0.9097\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.1445 - val_accuracy: 0.9167\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.1446 - val_accuracy: 0.9167\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0075 - accuracy: 0.9992 - val_loss: 0.1440 - val_accuracy: 0.9097\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0228 - accuracy: 0.9961 - val_loss: 0.1442 - val_accuracy: 0.9167\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0127 - accuracy: 0.9992 - val_loss: 0.1438 - val_accuracy: 0.9097\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 17s 107ms/step - loss: 0.0157 - accuracy: 0.9938 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.1443 - val_accuracy: 0.9097\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.1449 - val_accuracy: 0.9097\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.1442 - val_accuracy: 0.9097\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.1441 - val_accuracy: 0.9097\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.1439 - val_accuracy: 0.9097\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.9097\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9977 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.1447 - val_accuracy: 0.9097\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0067 - accuracy: 0.9992 - val_loss: 0.1442 - val_accuracy: 0.9167\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 0.1440 - val_accuracy: 0.9097\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.1439 - val_accuracy: 0.9097\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 0.1444 - val_accuracy: 0.9097\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0110 - accuracy: 0.9977 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0095 - accuracy: 0.9992 - val_loss: 0.1440 - val_accuracy: 0.9097\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.1429 - val_accuracy: 0.9097\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.1438 - val_accuracy: 0.9097\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0115 - accuracy: 0.9992 - val_loss: 0.1440 - val_accuracy: 0.9097\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.1438 - val_accuracy: 0.9097\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.1440 - val_accuracy: 0.9167\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9097\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.1447 - val_accuracy: 0.9097\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.1443 - val_accuracy: 0.9167\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.1455 - val_accuracy: 0.9167\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.1443 - val_accuracy: 0.9097\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1442 - val_accuracy: 0.9097\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0084 - accuracy: 0.9992 - val_loss: 0.1445 - val_accuracy: 0.9097\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0122 - accuracy: 0.9977 - val_loss: 0.1443 - val_accuracy: 0.9097\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 17s 106ms/step - loss: 0.0097 - accuracy: 0.9992 - val_loss: 0.1451 - val_accuracy: 0.9097\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 17s 105ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.1447 - val_accuracy: 0.9097\n",
      "Score for fold 10: loss of 0.14465723931789398; accuracy of 90.97222089767456%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2991652190685272 - Accuracy: 86.11111044883728%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.21841709315776825 - Accuracy: 87.5%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2520538568496704 - Accuracy: 88.88888955116272%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24562504887580872 - Accuracy: 88.19444179534912%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.20063143968582153 - Accuracy: 86.80555820465088%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.1776886284351349 - Accuracy: 89.58333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.2139722853899002 - Accuracy: 88.88888955116272%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.16198259592056274 - Accuracy: 92.36111044883728%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.28090229630470276 - Accuracy: 85.41666865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.14465723931789398 - Accuracy: 90.97222089767456%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 88.47222208976746 (+- 2.0412406817591346)\n",
      "> Loss: 0.21950957030057908\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/3.10/GAF/train/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/3.10/GAF/test/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(200, 200, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=8, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = callbacks_list) # 여기에 Validation set을 넣어야되는거 아닌가?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/3.10/GAF/weight/' + f'GAF_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
